{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, path\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(1, '/home/leonriccius/PycharmProjects/data_driven_rans/scripts/')\n",
    "from utilities import get_invariants, get_tensor_functions, mean_std_scaling, anisotropy, cap_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation on grid, cutting out boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = '/home/leonriccius/Documents/Fluid_Data/training_data/periodic_hills'\n",
    "# on workstation: home/leon/Master_Thesis/Fluid_Data ...\n",
    "rans_time = '1500'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = os.listdir(os.sep.join([train_data_path, 'tensordata']))\n",
    "print(cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion of the bottom boundary\n",
    "# characteristic points\n",
    "x = (1.929/54) * np.array([0., 0.1, 9., 14., 20., 30., 40., 53.9, 54.])\n",
    "x = np.append(x, 9 - x[::-1])\n",
    "y = (1.929/54) * np.array([28., 28., 27., 24., 19., 11., 4., 0., 0.])\n",
    "y = np.append(y, y[::-1])\n",
    "\n",
    "# spline interpolation\n",
    "f_bottom = interp1d(x, y, kind='cubic', fill_value='extrapolate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create mask for points selection\n",
    "def mask_boundary_points(x, y, blthickness = 0.15):\n",
    "    mask = np.ones(x.shape, dtype=bool)\n",
    "    y_interp = f_bottom(x)\n",
    "    mask[np.where(y < y_interp + blthickness)] = False\n",
    "    mask[np.where(y > 3.035 - blthickness)] = False\n",
    "    mask[np.where(x < 0. + blthickness)] = False\n",
    "    mask[np.where(x > 9. - blthickness)] = False\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define grid for interpolation\n",
    "grid_x, grid_y = np.mgrid[0.:9.:150j, 0.:3.05:50j]\n",
    "grid = th.tensor([grid_x.flatten(), grid_y.flatten()]).T\n",
    "\n",
    "# get mask for points selection\n",
    "mask = mask_boundary_points(grid_x.flatten(), grid_y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for rans data\n",
    "for case in cases:\n",
    "    # define current path and read in data\n",
    "    curr_case = os.sep.join([train_data_path, 'rans', case, rans_time])\n",
    "    rs_rans = th.load(os.sep.join([curr_case, 'RS-torch.th']))\n",
    "    s = th.load(os.sep.join([curr_case, 'S-torch.th']))\n",
    "    r = th.load(os.sep.join([curr_case, 'R-torch.th']))\n",
    "    epsilon0 = th.load(os.sep.join([curr_case, 'epsilon-torch.th']))\n",
    "    k0 = th.load(os.sep.join([curr_case, 'k-torch.th']))\n",
    "    grid = th.load(os.sep.join([curr_case, 'grid-torch.th']))[:,0:2]\n",
    "    \n",
    "    # expand k\n",
    "    k = k0.unsqueeze(0).unsqueeze(0).expand(3,3,k0.size()[0])\n",
    "    k = k.permute(2, 0, 1)\n",
    "    \n",
    "    # expand epsilon\n",
    "    epsilon = epsilon0.unsqueeze(0).unsqueeze(0).expand(3,3,epsilon0.size()[0])\n",
    "    epsilon = epsilon.permute(2, 0, 1)\n",
    "    \n",
    "    # normalize S and R\n",
    "    s_hat = k/epsilon*s\n",
    "    r_hat = k/epsilon*r\n",
    "    \n",
    "    # compute invariants and basis tensors\n",
    "    inv = get_invariants(s_hat, r_hat)\n",
    "    T = get_tensor_functions(s_hat, r_hat)\n",
    "    \n",
    "    # compute anisotropy tensor b\n",
    "    b_rans = rs_rans/(2*k) - 1/3 * th.eye(3).unsqueeze(0).expand(k0.shape[0],3,3)\n",
    "    \n",
    "    # interpolate inv, T\n",
    "    inv_interp = th.tensor(griddata(grid, inv, (grid_x, grid_y), method='linear')).flatten(end_dim = 1)\n",
    "    T_interp = th.tensor(griddata(grid, T, (grid_x, grid_y), method='linear')).flatten(end_dim = 1)\n",
    "    b_rans_interp = th.tensor(griddata(grid, b_rans, (grid_x, grid_y), method='linear')).flatten(end_dim = 1)\n",
    "    grid_interp = th.tensor([grid_x.flatten(), grid_y.flatten()]).T\n",
    "    \n",
    "    # remove points close to boundary\n",
    "    inv_interp = inv_interp[mask]\n",
    "    T_interp = T_interp[mask]\n",
    "    grid_interp = grid_interp[mask]\n",
    "    b_rans_interp = b_rans_interp[mask]\n",
    "    \n",
    "    # store inv, T, grid\n",
    "    storage_path = os.sep.join([train_data_path, 'unscaled_tensordata', case]) # 'tensordata'\n",
    "#     th.save(inv_interp, os.sep.join([storage_path, 'inv-torch.th']))\n",
    "#     th.save(T_interp, os.sep.join([storage_path, 'T-torch.th']))y\n",
    "#     th.save(b_rans_interp, os.sep.join([storage_path, 'b_rans-torch.th']))\n",
    "#     th.save(grid_interp, os.sep.join([storage_path, 'grid-torch.th']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for DNS data\n",
    "for case in cases:\n",
    "    # define current path and read in data\n",
    "    curr_case = os.sep.join([train_data_path, 'dns', case])\n",
    "    data = np.loadtxt(os.sep.join([curr_case, 'ASCII_Breuer.dat']), skiprows=16)\n",
    "    \n",
    "    # get shape of data\n",
    "    N, M = data.shape\n",
    "    \n",
    "    # extract relevant data from dataset\n",
    "    rs_dns = th.tensor(np.array([data[:,6], data[:,9], np.full(N,0),\n",
    "                                data[:,9], data[:,7], np.full(N,0),\n",
    "                                np.full(N,0), np.full(N,0), data[:,8]]).T.reshape(-1,3,3))\n",
    "    grid = th.tensor([data[:,0],data[:,1]]).T\n",
    "    k0 = 0.5*th.from_numpy(rs_dns.numpy().trace(axis1 = 1, axis2 = 2))\n",
    "    \n",
    "    # expand k\n",
    "    k = k0.unsqueeze(0).unsqueeze(0).expand(3,3,k0.shape[0])\n",
    "    k = k.permute(2, 0, 1)\n",
    "\n",
    "    # compute anisotropy tensor b\n",
    "    b_dns = rs_dns/(2*k) - 1/3 * th.eye(3).unsqueeze(0).expand(k0.shape[0],3,3)\n",
    "    \n",
    "    # interpolate b\n",
    "    b_dns_interp = th.tensor(griddata(grid, b_dns, (grid_x, grid_y), method='linear')).flatten(end_dim = 1)\n",
    "    \n",
    "    # remove points close to boundary\n",
    "    b_dns_interp = b_dns_interp[mask]\n",
    "    \n",
    "    # save b\n",
    "    storage_path = os.sep.join([train_data_path, 'unscaled_tensordata', case]) # 'tensordata'\n",
    "#     th.save(b_dns_interp, os.sep.join([storage_path, 'b_dns-torch.th']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_x, grid_y = np.mgrid[0.:9.:90j, 0.:3.05:30j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = mask_boundary_points(grid_x.flatten(), grid_y.flatten())\n",
    "grid_x_cut, grid_y_cut = (grid_x.flatten()[mask], grid_y.flatten()[mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate on RANS points, without cutting off boundary regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_path = '/home/leonriccius/Documents/Fluid_Data/training_data/periodic_hills'\n",
    "# # on workstation: home/leon/Master_Thesis/Fluid_Data ...\n",
    "# rans_time = '1500'\n",
    "\n",
    "# cases = os.listdir(os.sep.join([train_data_path, 'tensordata_rans_grid']))\n",
    "# print(cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2800', '1400', '700', '5600', '10595']\n"
     ]
    }
   ],
   "source": [
    "train_data_path = '/home/leonriccius/Documents/Fluid_Data/training_data/periodic_hills'\n",
    "# on workstation: home/leon/Master_Thesis/Fluid_Data ...\n",
    "# rans_time = '511'\n",
    "\n",
    "cases = os.listdir(os.sep.join([train_data_path, 'tensordata_normalized']))\n",
    "# cases = [cases[0]]\n",
    "print(cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_path = '/home/leonriccius/Documents/Fluid_Data/training_data/curved_backstep'\n",
    "# # on workstation: home/leon/Master_Thesis/Fluid_Data ...\n",
    "# # rans_time = '511'\n",
    "\n",
    "# cases = os.listdir(os.sep.join([train_data_path, 'tensordata']))\n",
    "# cases = [cases[0]]\n",
    "# print(cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir_list = os.listdir(os.sep.join([train_data_path, 'rans', cases[0]]))\n",
    "# numerics = [entry for entry in dir_list if entry.isnumeric()]\n",
    "# max([entry for entry in dir_list if entry.isnumeric()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fluid case: 2800\n",
      "s_hat max value: 2.902217\n",
      "r_hat max value: 2.141694\n",
      "check wether b_dns contains nan entries:  False\n",
      "check wether interpolated b contains nan entries:  False \n",
      "\n",
      "Fluid case: 1400\n",
      "s_hat max value: 2.871371\n",
      "r_hat max value: 2.140056\n",
      "check wether b_dns contains nan entries:  False\n",
      "check wether interpolated b contains nan entries:  False \n",
      "\n",
      "Fluid case: 700\n",
      "s_hat max value: 2.834470\n",
      "r_hat max value: 2.144192\n",
      "check wether b_dns contains nan entries:  False\n",
      "check wether interpolated b contains nan entries:  False \n",
      "\n",
      "Fluid case: 5600\n",
      "s_hat max value: 2.944608\n",
      "r_hat max value: 2.341564\n",
      "check wether b_dns contains nan entries:  False\n",
      "check wether interpolated b contains nan entries:  False \n",
      "\n",
      "Fluid case: 10595\n",
      "s_hat max value: 3.429326\n",
      "r_hat max value: 3.430111\n",
      "check wether b_dns contains nan entries:  False\n",
      "check wether interpolated b contains nan entries:  False \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loop over flow cases\n",
    "for case in cases:\n",
    "    \n",
    "    # load rans data and compute QoI\n",
    "    # define current path and read in data\n",
    "    curr_rans_dir = os.sep.join([train_data_path, 'rans', case])\n",
    "    rans_time = max([entry for entry in os.listdir(curr_rans_dir) if entry.isnumeric()])\n",
    "    curr_rans_case = os.sep.join([curr_rans_dir, rans_time])\n",
    "    \n",
    "    # load data\n",
    "    rs_rans = th.load(os.sep.join([curr_rans_case, 'RS-torch.th']))\n",
    "    s = th.load(os.sep.join([curr_rans_case, 'S-torch.th']))\n",
    "    r = th.load(os.sep.join([curr_rans_case, 'R-torch.th']))\n",
    "    epsilon0 = th.load(os.sep.join([curr_rans_case, 'epsilon-torch.th']))\n",
    "    k0_rans = th.load(os.sep.join([curr_rans_case, 'k-torch.th']))\n",
    "    grid_rans = th.load(os.sep.join([curr_rans_case, 'grid-torch.th']))[:,0:2]\n",
    "    \n",
    "    # expand k\n",
    "    k_rans = k0_rans.unsqueeze(0).unsqueeze(0).expand(3,3,k0_rans.size()[0])\n",
    "    k_rans = k_rans.permute(2, 0, 1)\n",
    "    \n",
    "    # expand epsilon\n",
    "    epsilon = epsilon0.unsqueeze(0).unsqueeze(0).expand(3,3,epsilon0.size()[0])\n",
    "    epsilon = epsilon.permute(2, 0, 1)\n",
    "    \n",
    "    # normalize S and R\n",
    "    s_hat = k_rans/epsilon*s\n",
    "    r_hat = k_rans/epsilon*r\n",
    "    \n",
    "    print('Fluid case: {}'.format(case))\n",
    "    print('s_hat max value: {:3f}'.format(th.max(th.abs(s_hat))))\n",
    "    print('r_hat max value: {:3f}'.format(th.max(th.abs(r_hat))))\n",
    "    \n",
    "    if th.max(th.abs(s_hat)) > 6. or th.max(th.abs(r_hat)) > 6.:\n",
    "        print('capping tensors ...')\n",
    "        s_hat = cap_tensor(s_hat, 6.0)\n",
    "        r_hat = cap_tensor(r_hat, 6.0)\n",
    "    \n",
    "    # compute invariants and basis tensors\n",
    "    inv = get_invariants(s_hat, r_hat)\n",
    "    T = get_tensor_functions(s_hat, r_hat)\n",
    "    \n",
    "    # scale invariants\n",
    "    inv = mean_std_scaling(inv) # alternative sigmoid_scaling\n",
    "    \n",
    "    # compute anisotropy tensor b\n",
    "    b_rans = rs_rans/(2*k_rans) - 1/3 * th.eye(3).unsqueeze(0).expand(k0_rans.shape[0],3,3)\n",
    "    \n",
    "    curr_case = os.sep.join([train_data_path, 'dns', case])\n",
    "    \n",
    "    # for periodic hills\n",
    "    data = np.loadtxt(os.sep.join([curr_case, 'ASCII_Breuer.dat']), skiprows=16)\n",
    "    \n",
    "    # load dns data and compute QoI\n",
    "    # get shape of data\n",
    "    N, M = data.shape\n",
    "\n",
    "    # extract relevant data from dataset\n",
    "    rs_dns = th.tensor(np.array([data[:,6], data[:,9], np.full(N,0),\n",
    "                                data[:,9], data[:,7], np.full(N,0),\n",
    "                                np.full(N,0), np.full(N,0), data[:,8]]).T.reshape(-1,3,3))\n",
    "    grid_dns = th.tensor([data[:,0],data[:,1]]).T\n",
    "    k0_dns = 0.5*th.from_numpy(rs_dns.numpy().trace(axis1 = 1, axis2 = 2))\n",
    "\n",
    "#     # for conv div channel\n",
    "#     rs_dns = th.load(os.sep.join([curr_case, 'RS-torch.th']))\n",
    "#     k0_dns = th.load(os.sep.join([curr_case, 'k-torch.th']))\n",
    "#     grid_dns = th.load(os.sep.join([curr_case, 'grid-torch.th'])) # [:,0:2]\n",
    "    \n",
    "    # find points with k = 0.0 and remove\n",
    "#     zero_trace_index = th.nonzero(k0_dns, as_tuple=True)\n",
    "#     print('removed {} zero trace points'.format(len(k0_dns) - len(zero_trace_index[0])))\n",
    "#     rs_dns = rs_dns[zero_trace_index]\n",
    "#     grid_dns = grid_dns[zero_trace_index]\n",
    "#     k0_dns = k0_dns[zero_trace_index]\n",
    "\n",
    "\n",
    "    # compute anisotropy tensor b\n",
    "    b_dns = anisotropy(rs_dns, k0_dns)\n",
    "    print('check wether b_dns contains nan entries: ', bool(th.max(th.isnan(b_dns))))\n",
    "\n",
    "    # interpolare b, rs, and k (might be neede later)\n",
    "    b_dns_interp = th.tensor(griddata(grid_dns[:,0:2],\n",
    "                                      b_dns,\n",
    "                                      (grid_rans[:,0], grid_rans[:,1]),\n",
    "                                      method='linear'))\n",
    "    rs_dns_interp = th.tensor(griddata(grid_dns[:,0:2],\n",
    "                                       rs_dns,\n",
    "                                       (grid_rans[:,0], grid_rans[:,1]),\n",
    "                                       method='linear'))\n",
    "    k0_dns_interp = th.tensor(griddata(grid_dns[:,0:2],\n",
    "                                       k0_dns,\n",
    "                                       (grid_rans[:,0], grid_rans[:,1]),\n",
    "                                       method='linear'))\n",
    "    print('check wether interpolated b contains nan entries: ', bool(th.max(th.isnan(b_dns_interp))), '\\n')\n",
    "    \n",
    "    # save all QoI\n",
    "    storage_path = os.sep.join([train_data_path, 'tensordata_normalized', case])\n",
    "    # save b\n",
    "    th.save(b_dns_interp, os.sep.join([storage_path, 'b_dns-torch.th']))\n",
    "    th.save(rs_dns_interp, os.sep.join([storage_path, 'rs_dns-torch.th']))\n",
    "    th.save(k0_dns_interp, os.sep.join([storage_path, 'k_dns-torch.th']))\n",
    "    \n",
    "    # save inv, T, grid\n",
    "    th.save(inv, os.sep.join([storage_path, 'inv-torch.th']))\n",
    "    th.save(T, os.sep.join([storage_path, 'T-torch.th']))\n",
    "    th.save(b_rans, os.sep.join([storage_path, 'b_rans-torch.th']))\n",
    "    th.save(grid_rans, os.sep.join([storage_path, 'grid-torch.th']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = th.isnan(b_dns_interp[:,0,0])\n",
    "# nan_cells = grid_rans[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_nonzero = th.nonzero(k0_dns==0.0, as_tuple=True)\n",
    "# nonzero_cells = grid_dns[index_nonzero]\n",
    "# nonzero_b = b_dns[index_nonzero]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bool(th.max(th.isnan(nonzero_b[:,0,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(index_nonzero[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(nonzero_cells[:,0], nonzero_cells[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = th.isnan(b_dns[:,0,0])\n",
    "nan_cells = grid_dns[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(nan_cells[:,0], nan_cells[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(grid_rans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "ax.scatter(grid_dns[:,0], grid_dns[:,1], c=b_dns[:,0,0], s=2.)\n",
    "# ax.scatter(grid_rans[:,0], grid_rans[:,1], c=b_dns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(grid_rans[:,0], grid_rans[:,1], c=b_dns_interp[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b_dns_interp[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case = cases[0]\n",
    "\n",
    "# curr_case = os.sep.join([train_data_path, 'rans', case, rans_time])\n",
    "# rs_rans = th.load(os.sep.join([curr_case, 'RS-torch.th']))\n",
    "# s = th.load(os.sep.join([curr_case, 'S-torch.th']))\n",
    "# r = th.load(os.sep.join([curr_case, 'R-torch.th']))\n",
    "# epsilon0 = th.load(os.sep.join([curr_case, 'epsilon-torch.th']))\n",
    "# k0 = th.load(os.sep.join([curr_case, 'k-torch.th']))\n",
    "# grid = th.load(os.sep.join([curr_case, 'grid-torch.th']))[:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loop for DNS data\n",
    "# for case in cases:\n",
    "#     curr_case = os.sep.join([train_data_path, 'dns', case])\n",
    "#     data = np.loadtxt(os.sep.join([curr_case, 'ASCII_Breuer.dat']), skiprows=16)\n",
    "\n",
    "#     # get shape of data\n",
    "#     N, M = data.shape\n",
    "\n",
    "#     # extract relevant data from dataset\n",
    "#     rs_dns = th.tensor(np.array([data[:,6], data[:,9], np.full(N,0),\n",
    "#                                 data[:,9], data[:,7], np.full(N,0),\n",
    "#                                 np.full(N,0), np.full(N,0), data[:,8]]).T.reshape(-1,3,3))\n",
    "#     grid_dns = th.tensor([data[:,0],data[:,1]]).T\n",
    "#     k0 = 0.5*th.from_numpy(rs_dns.numpy().trace(axis1 = 1, axis2 = 2))\n",
    "\n",
    "#     # expand k\n",
    "#     k = k0.unsqueeze(0).unsqueeze(0).expand(3,3,k0.shape[0])\n",
    "#     k = k.permute(2, 0, 1)\n",
    "\n",
    "#     # compute anisotropy tensor b\n",
    "#     b_dns = rs_dns/(2*k) - 1/3 * th.eye(3).unsqueeze(0).expand(k0.shape[0],3,3)\n",
    "\n",
    "#     # interpolare b\n",
    "#     b_dns_interp = th.tensor(griddata(grid_dns, b_dns, (grid[:,0], grid[:,1]), method='linear'))\n",
    "    \n",
    "#     # save b\n",
    "#     storage_path = os.sep.join([train_data_path, 'unscaled_tensordata', case]) # 'tensordata'\n",
    "# #    th.save(b_dns_interp, os.sep.join([storage_path, 'b_dns-torch.th']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18,5))\n",
    "\n",
    "ax.scatter(grid_x, grid_y, s=10)\n",
    "ax.scatter(grid_x_cut, grid_y_cut,s=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_all_x, grid_all_y = np.mgrid[0.:9.:300j, 0.:3.035:100j]\n",
    "grid_all_z1 = griddata(grid_interp, T_interp[:,4,1,1], (grid_all_x, grid_all_y), method='linear') # inv[:,1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,5))\n",
    "DNS_all_domain = ax.contourf(grid_all_x, grid_all_y, grid_all_z1)\n",
    "fig.colorbar(DNS_all_domain, ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
